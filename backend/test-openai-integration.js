#!/usr/bin/env node

const llmService = require('./services/llmService');
const qualityMetricsService = require('./services/qualityMetricsService');

/**
 * Test script for OpenAI integration and quality metrics
 * This demonstrates the core functionality required for the GenAI Labs Challenge
 */

async function testOpenAIIntegration() {
  console.log('üß™ Testing OpenAI Integration and Quality Metrics\n');

  const testPrompt = "Write a short creative story about a robot learning to paint.";
  
  // Test different parameter combinations
  const parameterCombinations = [
    { temperature: 0.3, top_p: 0.8, max_tokens: 150 },
    { temperature: 0.7, top_p: 0.9, max_tokens: 150 },
    { temperature: 1.0, top_p: 1.0, max_tokens: 150 }
  ];

  console.log(`üìù Test Prompt: "${testPrompt}"\n`);
  console.log(`üéõÔ∏è  Testing ${parameterCombinations.length} parameter combinations:\n`);

  const results = [];

  for (let i = 0; i < parameterCombinations.length; i++) {
    const params = parameterCombinations[i];
    console.log(`üîÑ Generating response ${i + 1}/3 with parameters:`, params);
    
    try {
      // Generate response
      const startTime = Date.now();
      const response = await llmService.generateResponse(testPrompt, params);
      const endTime = Date.now();
      
      console.log(`‚úÖ Response generated in ${endTime - startTime}ms`);
      console.log(`üìä Model: ${response.model}`);
      console.log(`üî¢ Token usage: ${response.usage.total_tokens} total (${response.usage.prompt_tokens} prompt + ${response.usage.completion_tokens} completion)`);
      
      // Calculate quality metrics
      const metrics = qualityMetricsService.calculateMetrics(response.content, testPrompt);
      
      console.log(`üìà Quality Metrics:`);
      console.log(`   ‚Ä¢ Coherence: ${metrics.coherence_score}/100`);
      console.log(`   ‚Ä¢ Completeness: ${metrics.completeness_score}/100`);
      console.log(`   ‚Ä¢ Readability: ${metrics.readability_score}/100`);
      console.log(`   ‚Ä¢ Length Appropriateness: ${metrics.length_appropriateness_score}/100`);
      console.log(`   ‚Ä¢ Overall Score: ${metrics.overall_score}/100`);
      
      console.log(`üìù Response Preview: "${response.content.substring(0, 100)}..."`);
      console.log(`üìä Text Stats: ${metrics.word_count} words, ${metrics.sentence_count} sentences\n`);
      
      results.push({
        parameters: params,
        response: response.content,
        metrics,
        usage: response.usage,
        responseTime: response.response_time
      });
      
    } catch (error) {
      console.error(`‚ùå Error generating response:`, error.message);
      console.log('');
    }
    
    // Add delay between requests to respect rate limits
    if (i < parameterCombinations.length - 1) {
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
  }

  // Analysis summary
  console.log('üìä EXPERIMENT SUMMARY');
  console.log('=' .repeat(50));
  
  if (results.length > 0) {
    const bestResponse = results.reduce((best, current) => 
      current.metrics.overall_score > best.metrics.overall_score ? current : best
    );
    
    console.log(`üèÜ Best Overall Response (Score: ${bestResponse.metrics.overall_score}):`);
    console.log(`   Temperature: ${bestResponse.parameters.temperature}`);
    console.log(`   Top-p: ${bestResponse.parameters.top_p}`);
    console.log(`   Response: "${bestResponse.response.substring(0, 150)}..."`);
    console.log('');
    
    // Average metrics
    const avgMetrics = {
      coherence: results.reduce((sum, r) => sum + r.metrics.coherence_score, 0) / results.length,
      completeness: results.reduce((sum, r) => sum + r.metrics.completeness_score, 0) / results.length,
      readability: results.reduce((sum, r) => sum + r.metrics.readability_score, 0) / results.length,
      overall: results.reduce((sum, r) => sum + r.metrics.overall_score, 0) / results.length
    };
    
    console.log(`üìà Average Quality Metrics Across All Responses:`);
    console.log(`   ‚Ä¢ Coherence: ${avgMetrics.coherence.toFixed(1)}`);
    console.log(`   ‚Ä¢ Completeness: ${avgMetrics.completeness.toFixed(1)}`);
    console.log(`   ‚Ä¢ Readability: ${avgMetrics.readability.toFixed(1)}`);
    console.log(`   ‚Ä¢ Overall: ${avgMetrics.overall.toFixed(1)}`);
    
  } else {
    console.log('‚ùå No successful responses generated');
  }

  console.log('\nüîß Service Status:');
  console.log(`   Mock Mode: ${llmService.isUsingMockMode()}`);
  if (llmService.isUsingMockMode()) {
    console.log('   ‚ö†Ô∏è  To test with real OpenAI API, add your API key to .env file');
  }

  console.log('\n‚úÖ Test completed!');
}

// Test connection first
async function testConnection() {
  console.log('üîå Testing LLM Service Connection...');
  
  try {
    const connectionResult = await llmService.testConnection();
    
    if (connectionResult.success) {
      if (connectionResult.mock) {
        console.log('‚úÖ Connected in MOCK mode');
        console.log('‚ö†Ô∏è  Add OPENAI_API_KEY to .env for real API testing\n');
      } else {
        console.log('‚úÖ Connected to OpenAI API');
        console.log(`üì° Model: ${connectionResult.model}\n`);
      }
      
      // Proceed with integration test
      await testOpenAIIntegration();
      
    } else {
      console.error('‚ùå Connection failed:', connectionResult.message);
    }
    
  } catch (error) {
    console.error('‚ùå Connection test failed:', error.message);
  }
}

// Run the test
if (require.main === module) {
  testConnection().catch(console.error);
}

module.exports = { testOpenAIIntegration, testConnection };
